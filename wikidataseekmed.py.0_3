"""
Wikidata医療用語抽出ツール（分割取得・ログ機能対応版）
実行方法: 
  python medical_terms_extractor.py --small --limit 2000 --log debug.log
  python medical_terms_extractor.py --medium --limit 5000 --batch-size 500 --log medium.log
"""

from SPARQLWrapper import SPARQLWrapper, JSON
import pandas as pd
import time
from datetime import datetime
import os
import argparse
from http.client import IncompleteRead
from urllib.error import URLError
import socket
import logging
import traceback

class MedicalTermsExtractor:
    def __init__(self, batch_size=1000, max_retries=5, log_file=None):
        self.sparql = SPARQLWrapper("https://query.wikidata.org/sparql")
        self.sparql.setReturnFormat(JSON)
        self.sparql.addCustomHttpHeader("User-Agent", "MedicalTermsExtractor/1.0")
        self.sparql.setTimeout(300)  # タイムアウトを5分に設定
        
        # 分割取得の設定
        self.batch_size = batch_size
        self.max_retries = max_retries
        
        # ログ設定
        self.setup_logging(log_file)
        
        # 統計情報
        self.stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'total_retries': 0,
            'total_items': 0,
        }
        
        # カテゴリー定義
        self.small_test_categories = {
            'Q12136': '病気',
            'Q12140': '医薬品',
            'Q169872': '症状',
            'Q796194': '外科手術',
            'Q1059392': '医学検査',
        }
        
        self.medium_test_categories = {
            'Q12136': '病気',
            'Q12140': '医薬品',
            'Q169872': '症状',
            'Q796194': '外科手術',
            'Q1059392': '医学検査',
            'Q8054': 'タンパク質',
            'Q7187': '遺伝子',
            'Q4936952': '解剖学的構造',
            'Q18123741': '感染症',
            'Q11173': '化学化合物',
            'Q2095549': '医療処置',
            'Q10876': '細菌',
            'Q808': 'ウイルス',
            'Q179661': '治療薬',
            'Q12139612': '医療機器',
        }
        
        self.large_test_categories = {
            'Q12136': '病気',
            'Q18123741': '感染症',
            'Q929833': '希少疾患',
            'Q18965518': '精神疾患',
            'Q18556609': '神経疾患',
            'Q4936952': '解剖学的構造',
            'Q24060765': '臓器',
            'Q28845870': '組織',
            'Q7644128': '細胞型',
            'Q12140': '医薬品',
            'Q179661': '治療薬',
            'Q128581': '抗生物質',
            'Q206159': 'ワクチン',
            'Q169872': '症状',
            'Q1441305': '医学的徴候',
            'Q7187': '遺伝子',
            'Q8054': 'タンパク質',
            'Q417841': 'タンパク質',
            'Q11173': '化学化合物',
            'Q59199015': '酵素',
            'Q178593': 'ホルモン',
            'Q1059392': '医学検査',
            'Q55788567': '画像診断',
            'Q11190': 'バイオマーカー',
            'Q796194': '外科手術',
            'Q2095549': '医療処置',
            'Q10876': '細菌',
            'Q808': 'ウイルス',
            'Q764': '真菌',
            'Q37763': '寄生虫',
            'Q12139612': '医療機器',
        }
    
    def setup_logging(self, log_file):
        """
        ログ機能の設定
        """
        if log_file:
            # ログディレクトリの作成
            log_dir = os.path.dirname(log_file)
            if log_dir and not os.path.exists(log_dir):
                os.makedirs(log_dir)
            
            # ロガーの設定
            self.logger = logging.getLogger('WikidataExtractor')
            self.logger.setLevel(logging.DEBUG)
            
            # ファイルハンドラ
            fh = logging.FileHandler(log_file, mode='w', encoding='utf-8')
            fh.setLevel(logging.DEBUG)
            
            # コンソールハンドラ（エラーのみ）
            ch = logging.StreamHandler()
            ch.setLevel(logging.ERROR)
            
            # フォーマット
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            fh.setFormatter(formatter)
            ch.setFormatter(formatter)
            
            self.logger.addHandler(fh)
            self.logger.addHandler(ch)
            
            self.logger.info("="*60)
            self.logger.info("Wikidata医療用語抽出ツール - ログ開始")
            self.logger.info("="*60)
            self.logger.info("ログファイル: " + log_file)
            self.logger.info("バッチサイズ: " + str(self.batch_size))
            self.logger.info("最大リトライ数: " + str(self.max_retries))
        else:
            # ログなしの場合はダミーロガー
            self.logger = logging.getLogger('WikidataExtractor')
            self.logger.addHandler(logging.NullHandler())
    
    def log_query(self, query, category_name, offset, batch_size):
        """
        クエリをログに記録
        """
        self.logger.info("")
        self.logger.info("-" * 60)
        self.logger.info("SPARQL Query Execution")
        self.logger.info("-" * 60)
        self.logger.info("Category: " + category_name)
        self.logger.info("Offset: " + str(offset))
        self.logger.info("Batch Size: " + str(batch_size))
        self.logger.info("Query Number: " + str(self.stats['total_queries'] + 1))
        self.logger.info("")
        self.logger.info("Query:")
        self.logger.info(query)
        self.logger.info("-" * 60)
    
    def log_response(self, bindings, elapsed_time):
        """
        レスポンスをログに記録
        """
        self.logger.info("")
        self.logger.info("Response:")
        self.logger.info("  Results: " + str(len(bindings)) + " items")
        self.logger.info("  Elapsed Time: " + str(round(elapsed_time, 2)) + " seconds")
        
        if len(bindings) > 0:
            self.logger.info("  Sample (first 3 items):")
            for i, binding in enumerate(bindings[:3]):
                item_id = binding.get('item', {}).get('value', '').split('/')[-1]
                en_label = binding.get('enLabel', {}).get('value', '')
                ja_label = binding.get('jaLabel', {}).get('value', '')
                self.logger.info("    [" + str(i+1) + "] " + item_id + " | EN: " + en_label + " | JA: " + ja_label)
        
        self.logger.info("-" * 60)
    
    def log_error(self, error, retry_count, category_name, offset):
        """
        エラーをログに記録
        """
        self.logger.error("")
        self.logger.error("!" * 60)
        self.logger.error("ERROR OCCURRED")
        self.logger.error("!" * 60)
        self.logger.error("Category: " + category_name)
        self.logger.error("Offset: " + str(offset))
        self.logger.error("Retry Attempt: " + str(retry_count))
        self.logger.error("Error Type: " + type(error).__name__)
        self.logger.error("Error Message: " + str(error))
        self.logger.error("")
        self.logger.error("Traceback:")
        self.logger.error(traceback.format_exc())
        self.logger.error("!" * 60)
    
    def execute_sparql_with_retry(self, query, category_name, offset, batch_size, retry_count=0):
        """
        リトライ機能付きSPARQLクエリ実行
        """
        self.stats['total_queries'] += 1
        
        # クエリをログに記録
        self.log_query(query, category_name, offset, batch_size)
        
        start_time = time.time()
        
        try:
            self.sparql.setQuery(query)
            
            self.logger.info("Sending HTTP Request to: https://query.wikidata.org/sparql")
            
            results = self.sparql.query().convert()
            
            elapsed_time = time.time() - start_time
            bindings = results["results"]["bindings"]
            
            # レスポンスをログに記録
            self.log_response(bindings, elapsed_time)
            
            self.stats['successful_queries'] += 1
            self.stats['total_items'] += len(bindings)
            
            return results
            
        except (IncompleteRead, URLError, socket.timeout) as e:
            elapsed_time = time.time() - start_time
            self.stats['failed_queries'] += 1
            
            # エラーをログに記録
            self.log_error(e, retry_count + 1, category_name, offset)
            
            if retry_count < self.max_retries:
                self.stats['total_retries'] += 1
                
                # エクスポネンシャルバックオフ
                wait_time = min(300, (2 ** retry_count) * 5)
                
                error_msg = "ネットワークエラー (試行 " + str(retry_count + 1) + "/" + str(self.max_retries) + "): " + type(e).__name__
                print(error_msg)
                wait_msg = str(wait_time) + "秒待機してリトライ..."
                print(wait_msg)
                
                self.logger.warning("Retrying after " + str(wait_time) + " seconds...")
                
                time.sleep(wait_time)
                return self.execute_sparql_with_retry(query, category_name, offset, batch_size, retry_count + 1)
            else:
                self.logger.error("Max retries reached. Giving up on this batch.")
                raise
                
        except Exception as e:
            elapsed_time = time.time() - start_time
            self.stats['failed_queries'] += 1
            
            # エラーをログに記録
            self.log_error(e, retry_count + 1, category_name, offset)
            
            if retry_count < self.max_retries:
                self.stats['total_retries'] += 1
                
                wait_time = (retry_count + 1) * 5
                
                error_msg = "エラー (試行 " + str(retry_count + 1) + "/" + str(self.max_retries) + "): " + str(e)
                print(error_msg)
                wait_msg = str(wait_time) + "秒待機してリトライ..."
                print(wait_msg)
                
                self.logger.warning("Retrying after " + str(wait_time) + " seconds...")
                
                time.sleep(wait_time)
                return self.execute_sparql_with_retry(query, category_name, offset, batch_size, retry_count + 1)
            else:
                self.logger.error("Max retries reached. Giving up on this batch.")
                raise
    
    def fetch_batch(self, category_qid, category_name, offset, batch_size):
        """
        指定オフセットから1バッチ分のデータを取得
        """
        query = """
        SELECT DISTINCT ?item ?enLabel ?jaLabel ?enDescription ?jaDescription 
               ?meshId ?icd10 ?icd9 ?snomedId ?umlsId
        WHERE {
          ?item wdt:P31/wdt:P279* wd:""" + category_qid + """ .
          
          ?item rdfs:label ?enLabel .
          FILTER(LANG(?enLabel) = "en")
          
          OPTIONAL {
            ?item rdfs:label ?jaLabel .
            FILTER(LANG(?jaLabel) = "ja")
          }
          
          OPTIONAL {
            ?item schema:description ?enDescription .
            FILTER(LANG(?enDescription) = "en")
          }
          
          OPTIONAL {
            ?item schema:description ?jaDescription .
            FILTER(LANG(?jaDescription) = "ja")
          }
          
          OPTIONAL { ?item wdt:P486 ?meshId }
          OPTIONAL { ?item wdt:P494 ?icd10 }
          OPTIONAL { ?item wdt:P493 ?icd9 }
          OPTIONAL { ?item wdt:P5806 ?snomedId }
          OPTIONAL { ?item wdt:P2892 ?umlsId }
        }
        LIMIT """ + str(batch_size) + """
        OFFSET """ + str(offset)
        
        results = self.execute_sparql_with_retry(query, category_name, offset, batch_size)
        return results["results"]["bindings"]
    
    def fetch_terms_by_category(self, category_qid, category_name, limit=None):
        """
        指定カテゴリーの医療用語を分割取得
        """
        self.logger.info("")
        self.logger.info("="*60)
        self.logger.info("Starting Category: " + category_name + " (" + category_qid + ")")
        self.logger.info("="*60)
        
        print("\n" + "="*60)
        print("カテゴリー: " + category_name)
        print("="*60)
        
        all_terms = []
        offset = 0
        consecutive_empty = 0
        max_empty_batches = 3
        
        effective_limit = limit if limit is not None else 1000000
        
        self.logger.info("Limit: " + str(effective_limit))
        self.logger.info("Batch Size: " + str(self.batch_size))
        
        while offset < effective_limit:
            try:
                remaining = effective_limit - offset
                current_batch_size = min(self.batch_size, remaining)
                
                progress_msg = "  取得中... offset=" + str(offset) + " (現在: " + str(len(all_terms)) + "件)"
                print(progress_msg, end='\r')
                
                # バッチ取得
                bindings = self.fetch_batch(category_qid, category_name, offset, current_batch_size)
                
                if not bindings:
                    consecutive_empty += 1
                    self.logger.warning("Empty batch received. Consecutive empty: " + str(consecutive_empty))
                    if consecutive_empty >= max_empty_batches:
                        empty_msg = "\n  連続して結果なし。このカテゴリーは終了。"
                        print(empty_msg)
                        self.logger.info("Category completed (consecutive empty batches)")
                        break
                    offset += current_batch_size
                    continue
                
                consecutive_empty = 0
                
                # データを変換
                for result in bindings:
                    term = {
                        'qid': result['item']['value'].split('/')[-1],
                        'category': category_name,
                        'category_qid': category_qid,
                        'en_label': result.get('enLabel', {}).get('value', ''),
                        'ja_label': result.get('jaLabel', {}).get('value', ''),
                        'en_description': result.get('enDescription', {}).get('value', ''),
                        'ja_description': result.get('jaDescription', {}).get('value', ''),
                        'mesh_id': result.get('meshId', {}).get('value', ''),
                        'icd10': result.get('icd10', {}).get('value', ''),
                        'icd9': result.get('icd9', {}).get('value', ''),
                        'snomed_id': result.get('snomedId', {}).get('value', ''),
                        'umls_id': result.get('umlsId', {}).get('value', ''),
                    }
                    all_terms.append(term)
                
                if len(bindings) < current_batch_size:
                    small_batch_msg = "\n  最後のバッチを取得完了。"
                    print(small_batch_msg)
                    self.logger.info("Last batch received (partial batch)")
                    break
                
                offset += current_batch_size
                time.sleep(1)
                
            except Exception as e:
                error_msg = "\n  バッチ取得失敗 (offset=" + str(offset) + "): " + str(e)
                print(error_msg)
                self.logger.error("Batch fetch failed. Skipping category.")
                print("  このカテゴリーをスキップして続行...")
                break
        
        count_msg = "\n  取得完了: " + str(len(all_terms)) + "件"
        print(count_msg)
        
        self.logger.info("")
        self.logger.info("Category Completed: " + category_name)
        self.logger.info("Total items collected: " + str(len(all_terms)))
        self.logger.info("="*60)
        
        return all_terms
    
    def extract_all(self, categories, limit_per_category=None):
        """
        全カテゴリーの医療用語を抽出
        """
        all_terms = []
        
        print("\n" + "="*60)
        cat_msg = "医療用語抽出開始: " + str(len(categories)) + "カテゴリー"
        print(cat_msg)
        if limit_per_category is None or limit_per_category == 0:
            print("各カテゴリー: 無制限（実用上限: 100万件）")
            print("バッチサイズ: " + str(self.batch_size) + "件")
        else:
            limit_msg = "各カテゴリー最大: " + str(limit_per_category) + "件"
            print(limit_msg)
            print("バッチサイズ: " + str(self.batch_size) + "件")
        print("="*60)
        
        self.logger.info("")
        self.logger.info("#"*60)
        self.logger.info("EXTRACTION START")
        self.logger.info("#"*60)
        self.logger.info("Total Categories: " + str(len(categories)))
        self.logger.info("Limit per Category: " + str(limit_per_category if limit_per_category else "Unlimited"))
        
        start_time = time.time()
        
        for idx, (qid, name) in enumerate(categories.items(), 1):
            cat_progress = "\n[" + str(idx) + "/" + str(len(categories)) + "] " + name
            print(cat_progress)
            
            terms = self.fetch_terms_by_category(qid, name, limit_per_category)
            all_terms.extend(terms)
            
            if idx < len(categories):
                time.sleep(2)
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "="*60)
        print("抽出完了")
        total_msg = "総取得件数: " + str(len(all_terms)) + "件"
        print(total_msg)
        time_msg = "所要時間: " + str(round(elapsed_time/60, 1)) + "分"
        print(time_msg)
        print("="*60 + "\n")
        
        # 統計情報をログに記録
        self.logger.info("")
        self.logger.info("#"*60)
        self.logger.info("EXTRACTION COMPLETED")
        self.logger.info("#"*60)
        self.logger.info("Total Items Collected: " + str(len(all_terms)))
        self.logger.info("Elapsed Time: " + str(round(elapsed_time/60, 1)) + " minutes")
        self.logger.info("")
        self.logger.info("Statistics:")
        self.logger.info("  Total Queries: " + str(self.stats['total_queries']))
        self.logger.info("  Successful Queries: " + str(self.stats['successful_queries']))
        self.logger.info("  Failed Queries: " + str(self.stats['failed_queries']))
        self.logger.info("  Total Retries: " + str(self.stats['total_retries']))
        self.logger.info("  Total Items Retrieved: " + str(self.stats['total_items']))
        if self.stats['total_queries'] > 0:
            success_rate = round(self.stats['successful_queries'] / self.stats['total_queries'] * 100, 1)
            self.logger.info("  Success Rate: " + str(success_rate) + "%")
        self.logger.info("#"*60)
        
        df = pd.DataFrame(all_terms)
        
        if len(df) == 0:
            print("警告: データが取得できませんでした。")
            self.logger.warning("No data collected!")
            return df
        
        original_count = len(df)
        df = df.drop_duplicates(subset=['qid'])
        duplicates_removed = original_count - len(df)
        
        if duplicates_removed > 0:
            dup_msg = "重複削除: " + str(duplicates_removed) + "件"
            print(dup_msg)
            unique_msg = "ユニーク件数: " + str(len(df)) + "件\n"
            print(unique_msg)
            
            self.logger.info("")
            self.logger.info("Duplicates removed: " + str(duplicates_removed))
            self.logger.info("Unique items: " + str(len(df)))
        
        return df
    
    def analyze_data_quality(self, df):
        """
        データ品質を分析
        """
        if len(df) == 0:
            print("データがありません。分析をスキップします。")
            return pd.DataFrame()
        
        print("\n" + "="*60)
        print("データ品質分析")
        print("="*60 + "\n")
        
        self.logger.info("")
        self.logger.info("="*60)
        self.logger.info("DATA QUALITY ANALYSIS")
        self.logger.info("="*60)
        
        print("1. 基本統計:")
        total_msg = "   総レコード数: " + str(len(df)) + "件"
        print(total_msg)
        self.logger.info("Total Records: " + str(len(df)))
        
        unique_msg = "   ユニークQID: " + str(df['qid'].nunique()) + "件"
        print(unique_msg)
        self.logger.info("Unique QIDs: " + str(df['qid'].nunique()))
        
        print("\n2. 言語カバレッジ:")
        has_en = (df['en_label'].notna() & (df['en_label'] != '')).sum()
        has_ja = (df['ja_label'].notna() & (df['ja_label'] != '')).sum()
        en_msg = "   英語ラベルあり: " + str(has_en) + "件 (" + str(round(has_en/len(df)*100, 1)) + "%)"
        print(en_msg)
        self.logger.info("English Labels: " + str(has_en) + " (" + str(round(has_en/len(df)*100, 1)) + "%)")
        
        ja_msg = "   日本語ラベルあり: " + str(has_ja) + "件 (" + str(round(has_ja/len(df)*100, 1)) + "%)"
        print(ja_msg)
        self.logger.info("Japanese Labels: " + str(has_ja) + " (" + str(round(has_ja/len(df)*100, 1)) + "%)")
        
        print("\n3. 説明文カバレッジ:")
        has_en_desc = (df['en_description'].notna() & (df['en_description'] != '')).sum()
        has_ja_desc = (df['ja_description'].notna() & (df['ja_description'] != '')).sum()
        en_desc_msg = "   英語説明あり: " + str(has_en_desc) + "件 (" + str(round(has_en_desc/len(df)*100, 1)) + "%)"
        print(en_desc_msg)
        
        ja_desc_msg = "   日本語説明あり: " + str(has_ja_desc) + "件 (" + str(round(has_ja_desc/len(df)*100, 1)) + "%)"
        print(ja_desc_msg)
        
        print("\n4. 外部IDカバレッジ:")
        external_ids = [
            ('mesh_id', 'MeSH'),
            ('icd10', 'ICD-10'),
            ('icd9', 'ICD-9'),
            ('snomed_id', 'SNOMED CT'),
            ('umls_id', 'UMLS')
        ]
        for col, name in external_ids:
            count = (df[col].notna() & (df[col] != '')).sum()
            ext_msg = "   " + name + ": " + str(count) + "件 (" + str(round(count/len(df)*100, 1)) + "%)"
            print(ext_msg)
            self.logger.info(name + ": " + str(count) + " (" + str(round(count/len(df)*100, 1)) + "%)")
        
        print("\n5. カテゴリー別件数:")
        self.logger.info("")
        self.logger.info("Category Breakdown:")
        category_counts = df['category'].value_counts().sort_values(ascending=False)
        for category, count in category_counts.items():
            cat_msg = "   " + category + ": " + str(count) + "件"
            print(cat_msg)
            self.logger.info("  " + category + ": " + str(count))
        
        print("\n6. 日英対訳可能な用語:")
        bilingual = df[(df['en_label'] != '') & (df['ja_label'] != '')]
        bi_msg = "   対訳ペア: " + str(len(bilingual)) + "件 (" + str(round(len(bilingual)/len(df)*100, 1)) + "%)"
        print(bi_msg)
        self.logger.info("")
        self.logger.info("Bilingual Pairs: " + str(len(bilingual)) + " (" + str(round(len(bilingual)/len(df)*100, 1)) + "%)")
        
        print("\n" + "="*60 + "\n")
        self.logger.info("="*60)
        
        return bilingual
    
    def save_results(self, df, prefix="small"):
        """
        結果をCSVとJSON形式で保存
        """
        if len(df) == 0:
            print("データがないため、保存をスキップします。")
            self.logger.warning("No data to save")
            return {}
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "output"
        
        os.makedirs(output_dir, exist_ok=True)
        
        print("結果を保存中...\n")
        self.logger.info("")
        self.logger.info("Saving results...")
        
        # 1. 完全版CSV
        full_csv = output_dir + "/" + prefix + "_medical_terms_full_" + timestamp + ".csv"
        df.to_csv(full_csv, index=False, encoding='utf-8-sig')
        file_size_mb = os.path.getsize(full_csv) / (1024 * 1024)
        full_msg = "   完全版CSV: " + full_csv + " (" + str(round(file_size_mb, 2)) + " MB)"
        print(full_msg)
        self.logger.info("Full CSV: " + full_csv + " (" + str(round(file_size_mb, 2)) + " MB)")
        
        # 2. 日英対訳のみCSV
        bilingual_df = df[(df['en_label'] != '') & (df['ja_label'] != '')].copy()
        if len(bilingual_df) > 0:
            bilingual_csv = output_dir + "/" + prefix + "_bilingual_dict_" + timestamp + ".csv"
            cols_to_save = ['en_label', 'ja_label', 'category', 'en_description', 'ja_description', 'qid']
            bilingual_df[cols_to_save].to_csv(bilingual_csv, index=False, encoding='utf-8-sig')
            file_size_mb = os.path.getsize(bilingual_csv) / (1024 * 1024)
            bi_msg = "   日英対訳CSV: " + bilingual_csv + " (" + str(len(bilingual_df)) + "件, " + str(round(file_size_mb, 2)) + " MB)"
            print(bi_msg)
            self.logger.info("Bilingual CSV: " + bilingual_csv + " (" + str(len(bilingual_df)) + " items, " + str(round(file_size_mb, 2)) + " MB)")
        
        # 3. カテゴリー別CSV
        category_dir = output_dir + "/by_category_" + timestamp
        os.makedirs(category_dir, exist_ok=True)
        for category in df['category'].unique():
            cat_df = df[df['category'] == category]
            safe_name = category.replace('/', '_').replace('\\', '_')
            cat_file = category_dir + "/" + safe_name + ".csv"
            cat_df.to_csv(cat_file, index=False, encoding='utf-8-sig')
        cat_count = len(df['category'].unique())
        cat_msg = "   カテゴリー別CSV: " + category_dir + "/ (" + str(cat_count) + "ファイル)"
        print(cat_msg)
        self.logger.info("Category CSVs: " + category_dir + "/ (" + str(cat_count) + " files)")
        
        # 4. JSON形式
        json_file = output_dir + "/" + prefix + "_medical_terms_" + timestamp + ".json"
        df.to_json(json_file, orient='records', force_ascii=False, indent=2)
        file_size_mb = os.path.getsize(json_file) / (1024 * 1024)
        json_msg = "   JSON: " + json_file + " (" + str(round(file_size_mb, 2)) + " MB)"
        print(json_msg)
        self.logger.info("JSON: " + json_file + " (" + str(round(file_size_mb, 2)) + " MB)")
        
        # 5. サマリーレポート
        report_file = output_dir + "/" + prefix + "_report_" + timestamp + ".txt"
        bilingual_count = len(bilingual_df) if len(bilingual_df) > 0 else 0
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("Wikidata医療用語抽出レポート\n")
            f.write("="*60 + "\n\n")
            f.write("実行日時: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + "\n")
            f.write("規模: " + prefix + "\n")
            f.write("総件数: " + str(len(df)) + "件\n")
            f.write("日英対訳: " + str(bilingual_count) + "件\n")
            if len(df) > 0:
                f.write("日英対訳率: " + str(round(bilingual_count/len(df)*100, 1)) + "%\n\n")
            
            f.write("統計情報:\n")
            f.write("  総クエリ数: " + str(self.stats['total_queries']) + "\n")
            f.write("  成功: " + str(self.stats['successful_queries']) + "\n")
            f.write("  失敗: " + str(self.stats['failed_queries']) + "\n")
            f.write("  リトライ回数: " + str(self.stats['total_retries']) + "\n\n")
            
            f.write("カテゴリー別件数:\n")
            for category, count in df['category'].value_counts().items():
                f.write("  " + category + ": " + str(count) + "件\n")
            
            f.write("\n言語カバレッジ:\n")
            has_ja = (df['ja_label'].notna() & (df['ja_label'] != '')).sum()
            ja_pct = round(has_ja/len(df)*100, 1) if len(df) > 0 else 0
            f.write("  日本語ラベルあり: " + str(has_ja) + "件 (" + str(ja_pct) + "%)\n")
            
            f.write("\n外部IDカバレッジ:\n")
            external_ids = [
                ('mesh_id', 'MeSH'),
                ('icd10', 'ICD-10'),
                ('snomed_id', 'SNOMED CT'),
                ('umls_id', 'UMLS')
            ]
            for col, name in external_ids:
                count = (df[col].notna() & (df[col] != '')).sum()
                pct = round(count/len(df)*100, 1) if len(df) > 0 else 0
                f.write("  " + name + ": " + str(count) + "件 (" + str(pct) + "%)\n")
        
        report_msg = "   レポート: " + report_file
        print(report_msg)
        self.logger.info("Report: " + report_file)
        
        print("\n保存完了！\n")
        self.logger.info("")
        self.logger.info("All results saved successfully")
        
        return {
            'full_csv': full_csv,
            'bilingual_csv': bilingual_csv if len(bilingual_df) > 0 else None,
            'category_dir': category_dir,
            'json': json_file,
            'report': report_file
        }


def parse_arguments():
    """
    コマンドライン引数を解析
    """
    parser = argparse.ArgumentParser(
        description='Wikidata医療用語抽出ツール（分割取得・ログ機能対応版）',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
使用例:
  小規模テスト（ログあり）:
    python %(prog)s --small --limit 2000 --log logs/small_test.log
  
  中規模テスト（ログあり、バッチサイズ調整）:
    python %(prog)s --medium --limit 5000 --batch-size 500 --log logs/medium_test.log
  
  大規模テスト（ログあり）:
    python %(prog)s --large --limit 0 --log logs/large_test.log
  
  ログなしで実行:
    python %(prog)s --small --limit 2000
        """
    )
    
    # サイズオプション
    size_group = parser.add_mutually_exclusive_group(required=True)
    size_group.add_argument('--small', action='store_true', help='小規模テスト（5カテゴリー）')
    size_group.add_argument('--medium', action='store_true', help='中規模テスト（15カテゴリー）')
    size_group.add_argument('--large', action='store_true', help='大規模テスト（30+カテゴリー）')
    
    # その他のオプション
    parser.add_argument('--limit', type=int, default=2000, 
                       help='各カテゴリーの最大取得件数（0で無制限、デフォルト: 2000）')
    parser.add_argument('--batch-size', type=int, default=1000,
                       help='1回のクエリで取得する件数（デフォルト: 1000）')
    parser.add_argument('--log', type=str, default=None,
                       help='ログファイルのパス（例: logs/debug.log）')
    parser.add_argument('--no-interactive', action='store_true',
                       help='対話モードをスキップして即座に実行')
    
    return parser.parse_args()


def main():
    """
    メイン処理
    """
    args = parse_arguments()
    
    # サイズとprefixを決定
    if args.small:
        size_name = "small"
        size_label = "小規模"
        category_count = 5
    elif args.medium:
        size_name = "medium"
        size_label = "中規模"
        category_count = 15
    else:
        size_name = "large"
        size_label = "大規模"
        category_count = "30+"
    
    limit_label = "無制限" if args.limit == 0 else str(args.limit) + "件"
    
    print("""
╔══════════════════════════════════════════════════════════╗
║   Wikidata医療用語抽出ツール（分割取得・ログ機能対応）  ║
╚══════════════════════════════════════════════════════════╝
    """)
    
    print("実行設定:")
    print("  規模: " + size_label + " (" + str(category_count) + "カテゴリー)")
    print("  各カテゴリー: " + limit_label)
    print("  バッチサイズ: " + str(args.batch_size) + "件")
    if args.log:
        print("  ログファイル: " + args.log)
    else:
        print("  ログファイル: なし")
    print("")
    
    # 推定時間
    if args.small and args.limit <= 2000:
        est_time = "10-20分"
    elif args.medium and args.limit <= 5000:
        est_time = "1-2時間"
    elif args.large or args.limit == 0:
        est_time = "数時間以上"
    else:
        est_time = "状況により変動"
    
    print("予想所要時間: " + est_time)
    print("")
    
    if not args.no_interactive:
        input("Enterキーを押して開始...")
    
    # Extractorを初期化
    extractor = MedicalTermsExtractor(
        batch_size=args.batch_size,
        max_retries=5,
        log_file=args.log
    )
    
    # カテゴリーを選択
    if args.small:
        categories = extractor.small_test_categories
    elif args.medium:
        categories = extractor.medium_test_categories
    else:
        categories = extractor.large_test_categories
    
    limit = None if args.limit == 0 else args.limit
    
    # データ抽出
    df = extractor.extract_all(categories, limit_per_category=limit)
    
    # データ品質分析
    bilingual_df = extractor.analyze_data_quality(df)
    
    # サンプル表示
    if len(bilingual_df) > 0:
        print("サンプルデータ（日英対訳あり、最初の5件）:")
        print("="*80)
        sample = bilingual_df[['en_label', 'ja_label', 'category']].head()
        for idx, row in sample.iterrows():
            en_part = row['en_label'][:30].ljust(30)
            ja_part = row['ja_label'][:20].ljust(20)
            cat_part = "[" + row['category'] + "]"
            sample_line = "  " + en_part + " -> " + ja_part + " " + cat_part
            print(sample_line)
        print("="*80 + "\n")
    
    # 結果保存
    files = extractor.save_results(df, prefix=size_name)
    
    print("""
╔══════════════════════════════════════════════════════════╗
║   抽出完了！                                              ║
╚══════════════════════════════════════════════════════════╝

次のステップ:
1. outputフォルダ内のCSVファイルを確認
2. データ品質を評価
""")
    
    if args.log:
        print("3. ログファイルを確認してデバッグ情報を確認: " + args.log)
    
    print("""
規模を変更する場合:
  小規模: python """ + __file__ + """ --small --limit 2000 --log logs/small.log
  中規模: python """ + __file__ + """ --medium --limit 5000 --log logs/medium.log
  大規模: python """ + __file__ + """ --large --limit 0 --log logs/large.log
    """)


if __name__ == "__main__":
    main()